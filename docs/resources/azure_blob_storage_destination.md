---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "mezmo_azure_blob_storage_destination Resource - terraform-provider-mezmo"
subcategory: ""
description: |-
  Publishes events to Azure Blob Storage
---

# mezmo_azure_blob_storage_destination (Resource)

Publishes events to Azure Blob Storage

## Example Usage

```terraform
terraform {
  required_providers {
    mezmo = {
      source = "registry.terraform.io/mezmo/mezmo"
    }
  }
  required_version = ">= 1.1.0"
}

provider "mezmo" {
  auth_key = "my secret"
}

resource "mezmo_pipeline" "pipeline1" {
  title = "My pipeline"
}

resource "mezmo_demo_source" "source1" {
  pipeline_id = mezmo_pipeline.pipeline1.id
  title       = "My source"
  description = "This is the point of entry for our data"
  format      = "nginx"
}

resource "mezmo_azure_blob_storage_destination" "destination1" {
  pipeline_id       = mezmo_pipeline.pipeline1.id
  title             = "My destination"
  description       = "Send logs to Azure Blob Storage"
  inputs            = [mezmo_demo_source.source1.id]
  connection_string = "AccountName=mylogstorage;AccountKey=storageaccountkeybase64encoded;EndpointSuffix=core.windows.net"
  container_name    = "my-logs"
  compression       = "gzip"
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `connection_string` (String, Sensitive) A connection string for the account that contains an access key
- `container_name` (String) The name of the container for blob storage
- `pipeline_id` (String) The uuid of the pipeline

### Optional

- `ack_enabled` (Boolean) Acknowledge data from the source when it reaches the destination
- `batch_timeout_secs` (Number) The maximum amount of time, in seconds, events will be buffered before being flushed to the destination
- `compression` (String) The compression strategy used on the encoded data prior to sending
- `description` (String) A user-defined value describing the destination
- `encoding` (String) The encoding to apply to the data
- `file_consolidation` (Attributes) This sink writes many small files out to azure blob storage. Enabling this process will allow the automatic consolidation of these small files into larger files of your choosing. This process will enable upon deployment and run on the chosen interval from `Processing Interval` creating files named `merged_[timestamp].log` where `timestamp` is the time since epoch when the actual file was created. The process will recursively access all files under the `Base Path`  to handle merging sub-directory logging structures. (see [below for nested schema](#nestedatt--file_consolidation))
- `inputs` (List of String) The ids of the input components
- `prefix` (String) A prefix to be applied to all object keys
- `title` (String) A user-defined title for the destination

### Read-Only

- `generation_id` (Number) An internal field used for component versioning
- `id` (String) The uuid of the destination

<a id="nestedatt--file_consolidation"></a>
### Nested Schema for `file_consolidation`

Optional:

- `base_path` (String) The path from the container to begin recursively looking for files to merge. A blank value indicates root. Merged files will only contain data from the respective folder.
- `enabled` (Boolean) Toggles whether the process is enabled.
- `process_every_seconds` (Number) How often to run the consolidation process in seconds
- `requested_size_bytes` (Number) The requested size of the consolidated files in bytes.
